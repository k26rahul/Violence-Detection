{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k26ra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\k26ra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture(filename, timesep, rgb, h, w):\n",
    "  frames = np.zeros((timesep, rgb, h, w), dtype=float)\n",
    "  i = 0\n",
    "  vc = cv2.VideoCapture(filename)\n",
    "  if vc.isOpened():\n",
    "    rval, frame = vc.read()\n",
    "    while rval and i < timesep:\n",
    "      # Resize the frame to the specified width and height\n",
    "      frm = cv2.resize(frame, (w, h))\n",
    "      frm = np.expand_dims(frm, axis=0)\n",
    "      frm = np.moveaxis(frm, -1, 1)\n",
    "      if np.max(frm) > 1:\n",
    "        frm = frm / 255.0\n",
    "      frames[i][:] = frm\n",
    "      i += 1\n",
    "      rval, frame = vc.read()\n",
    "    vc.release()  # Release the video capture object\n",
    "  else:\n",
    "    print(\"Error: Unable to open video file\")\n",
    "  return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeWarp(nn.Module):\n",
    "  '''video to frames'''\n",
    "\n",
    "  def __init__(self, baseModel, method='sqeeze'):\n",
    "    super(TimeWarp, self).__init__()\n",
    "    self.baseModel = baseModel\n",
    "    self.method = method\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, time_steps, C, H, W = x.size()\n",
    "    if self.method == 'loop':\n",
    "      output = []\n",
    "      for i in range(time_steps):\n",
    "        # input one frame at a time into the basemodel\n",
    "        x_t = self.baseModel(x[:, i, :, :, :])\n",
    "        # Flatten the output\n",
    "        x_t = x_t.view(x_t.size(0), -1)\n",
    "        output.append(x_t)\n",
    "      # end loop\n",
    "      # make output as  ( samples, timesteps, output_size)\n",
    "      x = torch.stack(output, dim=0).transpose_(0, 1)\n",
    "      output = None  # clear var to reduce data  in memory\n",
    "      x_t = None  # clear var to reduce data  in memory\n",
    "    else:\n",
    "      # reshape input  to be (batch_size * timesteps, input_size)\n",
    "      x = x.contiguous().view(batch_size * time_steps, C, H, W)\n",
    "      x = self.baseModel(x)\n",
    "      x = x.view(x.size(0), -1)\n",
    "      # make output as  ( samples, timesteps, output_size)\n",
    "      x = x.contiguous().view(batch_size, time_steps, x.size(-1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extractlastcell(nn.Module):\n",
    "  def forward(self, x):\n",
    "    out, _ = x\n",
    "    return out[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(weight='./Statemamonmixed96accviolance.pth'):\n",
    "  # Create model\n",
    "  num_classes = 1\n",
    "  dr_rate = 0.2\n",
    "  pretrained = True\n",
    "  rnn_hidden_size = 80\n",
    "  rnn_num_layers = 1\n",
    "  baseModel = models.vgg19(pretrained=pretrained).features\n",
    "\n",
    "  i = 0\n",
    "  for child in baseModel.children():\n",
    "    if i < 28:\n",
    "      for param in child.parameters():\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "      for param in child.parameters():\n",
    "        param.requires_grad = True\n",
    "    i += 1\n",
    "\n",
    "  num_features = 12800\n",
    "  # Example of using Sequential\n",
    "  model = nn.Sequential(\n",
    "      TimeWarp(baseModel, method='loop'),\n",
    "      nn.LSTM(num_features, rnn_hidden_size, rnn_num_layers,\n",
    "              batch_first=True, bidirectional=True),\n",
    "      extractlastcell(),\n",
    "      nn.Linear(160, 256),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(dr_rate),\n",
    "      nn.Linear(256, num_classes)\n",
    "  )\n",
    "  checkpoint = torch.load(weight, map_location=torch.device('cpu'))\n",
    "  model.load_state_dict(checkpoint['state_dict'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_fight(model, video, acuracy=0.58):\n",
    "  model.eval()\n",
    "  outputs = model(video)\n",
    "  torch.sigmoid(outputs)\n",
    "  preds = torch.sigmoid(outputs).to('cpu')\n",
    "  preds = preds.detach().numpy()\n",
    "  if preds[0][0] >= acuracy:\n",
    "    return True, preds[0][0]\n",
    "  else:\n",
    "    return False, preds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): TimeWarp(\n",
       "    (baseModel): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): ReLU(inplace=True)\n",
       "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (33): ReLU(inplace=True)\n",
       "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (1): LSTM(12800, 80, batch_first=True, bidirectional=True)\n",
       "  (2): extractlastcell()\n",
       "  (3): Linear(in_features=160, out_features=256, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.2, inplace=False)\n",
       "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "device = torch.device(\"cpu\")   # Use CPU only\n",
    "model22 = Model()\n",
    "model22.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\n",
    "    './Meanwhile In New York City (Caught On Ring Camera) #shorts.mp4')\n",
    "\n",
    "cap.isOpened()\n",
    "\n",
    "# _1st_frame = cap.read()[1]\n",
    "# _1st_frame.shape\n",
    "\n",
    "count = 0\n",
    "while cap.read()[0]:\n",
    "  count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = None\n",
    "\n",
    "\n",
    "def main_fight(video_path, accuracyfight=0.65):\n",
    "  global vid\n",
    "\n",
    "  tmp_obj = {}\n",
    "\n",
    "  if os.path.exists('./tmp.mp4'):\n",
    "    os.remove('./tmp.mp4')\n",
    "\n",
    "  # Open the local video file\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "  # display(cap, dir(cap))\n",
    "\n",
    "  # Check if the video file was opened successfully\n",
    "  if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video file\")\n",
    "    return None\n",
    "\n",
    "  else:\n",
    "    # Open a file to write the video data\n",
    "    with open(\"tmp.mp4\", \"wb\") as file:\n",
    "      while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        # Check if the frame was read successfully\n",
    "        if not ret:\n",
    "          break  # Break the loop if no more frames are available\n",
    "        # Write the frame data to the file\n",
    "        file.write(frame)\n",
    "\n",
    "  vid = capture(\"tmp.mp4\", 40, 3, 170, 170)\n",
    "\n",
    "  # display(vid)\n",
    "\n",
    "  # return\n",
    "\n",
    "  v = np.array([vid])\n",
    "\n",
    "  v = torch.from_numpy(v)\n",
    "  v = v.to(device, dtype=torch.float)\n",
    "  millis = int(round(time.time() * 1000))\n",
    "  f, precent = pred_fight(model22, v, acuracy=accuracyfight)\n",
    "  tmp_obj = {'fight': f, 'precentegeoffight': str(precent)}\n",
    "  millis2 = int(round(time.time() * 1000))\n",
    "  tmp_obj['processing_time'] = str((millis2-millis)/1000)\n",
    "  return tmp_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid[0]\n",
    "# vid[39]\n",
    "#\n",
    "# vid[0][0]\n",
    "# vid[0][1]\n",
    "# vid[0][2]\n",
    "\n",
    "# vid[0][0][169][169]\n",
    "# vid[0][1][169][169]\n",
    "# vid[0][2][169][169]\n",
    "\n",
    "# vid[1][0][169][169]\n",
    "# vid[1][1][169][169]\n",
    "# vid[1][2][169][169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mvid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "# np.sum(vid.flatten() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([170, 170])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([170, 170])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([170, 170])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 3, 170, 170])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display(vid.shape)\n",
    "# v.shape\n",
    "\n",
    "# v = np.array([vid])\n",
    "# v = torch.from_numpy(v)\n",
    "# v[0][0][0].shape\n",
    "# v[0][0][1].shape\n",
    "# v[0][0][2].shape\n",
    "\n",
    "# v[0][0][2].shape\n",
    "\n",
    "# v[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_fight(\n",
    "#     video_path=\"./Meanwhile In New York City (Caught On Ring Camera) #shorts.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_fight(video_path, accuracyfight=0.65):\n",
    "  tmp_obj = {}\n",
    "\n",
    "  if os.path.exists('./tmp.mp4'):\n",
    "    os.remove('./tmp.mp4')\n",
    "\n",
    "  # Open the local video file\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "  # Check if the video file was opened successfully\n",
    "  if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video file\")\n",
    "    return None\n",
    "\n",
    "  else:\n",
    "    # Open a file to write the video data\n",
    "    with open(\"tmp.mp4\", \"wb\") as file:\n",
    "      while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        # Check if the frame was read successfully\n",
    "        if not ret:\n",
    "          break  # Break the loop if no more frames are available\n",
    "        # Write the frame data to the file\n",
    "        file.write(frame)\n",
    "\n",
    "  vid = capture(\"tmp.mp4\", 40, 3, 170, 170)\n",
    "  v = np.array([vid])\n",
    "\n",
    "  # boom = v.flatten() == 0\n",
    "  # print('ok: ', np.sum(boom) == len(boom))\n",
    "\n",
    "  v = torch.from_numpy(v)\n",
    "  v = v.to(device, dtype=torch.float)\n",
    "  millis = int(round(time.time() * 1000))\n",
    "  f, precent = pred_fight(model22, v, acuracy=accuracyfight)\n",
    "  tmp_obj = {'fight': f, 'precentegeoffight': str(precent)}\n",
    "  millis2 = int(round(time.time() * 1000))\n",
    "  tmp_obj['processing_time'] = str((millis2-millis)/1000)\n",
    "  return tmp_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to open video file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fight': True, 'precentegeoffight': '0.66339463', 'processing_time': '4.446'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_fight(\n",
    "    video_path='./Meanwhile In New York City (Caught On Ring Camera) #shorts.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
