{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassifier(nn.Module):\n",
    "  def __init__(self, lstm_input_size, lstm_hidden_size, lstm_num_layers, num_classes):\n",
    "    super(VideoClassifier, self).__init__()\n",
    "    self.mobilenet = models.mobilenet_v2(weights='DEFAULT')\n",
    "    self.lstm_input_size = lstm_input_size\n",
    "    self.lstm_hidden_size = lstm_hidden_size\n",
    "    self.lstm_num_layers = lstm_num_layers\n",
    "\n",
    "    # Freeze MobileNetV2 layers so they don't get trained.\n",
    "    for param in self.mobilenet.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    # making last layer identity(output = input), effectively making last layer numb\n",
    "    self.mobilenet.classifier = nn.Identity()\n",
    "\n",
    "    # making lstm network\n",
    "    self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_size,\n",
    "                        lstm_num_layers, batch_first=True, dropout=0.2)\n",
    "\n",
    "    # making FC layer for binary prediction\n",
    "    self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # pass x through mobilenet to extract features\n",
    "    features = self.mobilenet(x)\n",
    "\n",
    "    # reshaping features for lstm input\n",
    "    features = features.view(x.size(0), -1, features.size(1))\n",
    "\n",
    "    # passing through lstm layers\n",
    "    lstm_out, _ = self.lstm(features)\n",
    "\n",
    "    # tooking output from last time step\n",
    "    lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "    # passing through fc to get final output\n",
    "    output = self.fc(lstm_out)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1280  # Output size of MobileNetV2\n",
    "hidden_size = 256  # Size of hidden state in LSTM\n",
    "# Number of layers in LSTM (dropout expects more than 1 layers)\n",
    "num_layers = 2\n",
    "num_classes = 2    # Binary prediction\n",
    "\n",
    "model = VideoClassifier(input_size, hidden_size,\n",
    "                        num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories1 = ['violent', 'non-violent']\n",
    "categories2 = ['cam1', 'cam2']\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for category in categories1:\n",
    "  for typex in categories2:\n",
    "\n",
    "    path = os.path.join('Dataset', category, typex)\n",
    "    label = categories1.index(category)\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "      videos = os.path.join(path, file)\n",
    "\n",
    "      data.append([videos, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Dataset\\\\violent\\\\cam1\\\\47.mp4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for features, label in data:\n",
    "  x.append(features)\n",
    "  y.append(label)\n",
    "\n",
    "len(x)\n",
    "len(y)\n",
    "x[0]\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = ['video1.mp4', 'video2.mp4']  # List of video file paths\n",
    "# y = [1, 0]  # Corresponding labels (0 for violence, 1 for non-violence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)\n",
    "len(y_train)\n",
    "# x_train[90]\n",
    "# y_train[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "  def __init__(self, video_paths, labels, max_frame, transform=None):\n",
    "    self.video_paths = video_paths\n",
    "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    self.max_frame = max_frame\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.video_paths)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    video_path = self.video_paths[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    # read video_frames for each vid\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        i += 1\n",
    "        print(f'finished reading frames of video : {i}')\n",
    "        break\n",
    "      # converting color from bgr to rgb\n",
    "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Truncate or pad frames to ensure fixed length\n",
    "    if len(frames) < self.max_frame:\n",
    "      # Pad frames with zeros\n",
    "      frames += [np.zeros_like(frames[0])] * (self.max_frame - len(frames))\n",
    "    elif len(frames) > self.max_frame:\n",
    "      # Truncate frames\n",
    "      frames = frames[:self.max_frame]\n",
    "\n",
    "    # applying tranform\n",
    "    if self.transform:\n",
    "      frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "    # # Stack frames into a tensor\n",
    "    # frames_tensor = torch.stack(frames)\n",
    "\n",
    "    # Convert frames to tensor and move to GPU\n",
    "    frames_tensor = torch.stack(frames).to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    return frames_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frame = 300\n",
    "\n",
    "train_dataset = VideoDataset(x_train, y_train, max_frame, transform=transform)\n",
    "test_dataset = VideoDataset(x_test, y_test, max_frame, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.lstm.parameters()},\n",
    "    {'params': model.fc.parameters()}\n",
    "], lr=1e-3)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4325634\n",
      "2101762\n"
     ]
    }
   ],
   "source": [
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel()\n",
    "                       for p in model.parameters() if p.requires_grad)\n",
    "print(total_params)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (frames, labels) in enumerate(train_dataloader):\n",
    "#   print(frames.shape)\n",
    "#   print(labels.shape)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight file exists. Skipping training.\n",
      "Model weights loaded from existing file.\n"
     ]
    }
   ],
   "source": [
    "force_train = False\n",
    "num_epochs = 2\n",
    "start_epoch = num_epochs\n",
    "\n",
    "\n",
    "# Check if model_weight.pth exists\n",
    "model_weight_file = 'model_weight.pth'\n",
    "\n",
    "if os.path.exists(model_weight_file):\n",
    "  model.load_state_dict(torch.load(model_weight_file))\n",
    "  # If model_weight.pth exists and no force flag is set, skip training\n",
    "  if not force_train:\n",
    "    print(\"Model weight file exists. Skipping training.\")\n",
    "    print(\"Model weights loaded from existing file.\")\n",
    "  else:\n",
    "    print(\"Previous model_weight.pth file found.\")\n",
    "    print(\"Continuing training from previous state.\")\n",
    "    start_epoch = 0\n",
    "else:\n",
    "  # If model_weight.pth doesn't exist, start training\n",
    "  print(\"No previous model_weight.pth file found. Starting training.\")\n",
    "  start_epoch = 0\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "  model.train()\n",
    "  train_loss = 0.0\n",
    "  for batch_idx, (frames, labels) in enumerate(train_dataloader):\n",
    "\n",
    "    frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "    # reshaping features for model input\n",
    "    batch_size, num_frames, channels, height, width = frames.size()\n",
    "    reshaped_frames = frames.view(\n",
    "        batch_size * num_frames, channels, height, width)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(reshaped_frames)\n",
    "\n",
    "    # Reshape output to split batch and frame dimensions\n",
    "    reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "    probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "    # Aggregate predictions for each video\n",
    "    aggregated_probabilities = probabilities.mean(dim=1)\n",
    "    # Get the predicted class for each video\n",
    "    _, predicted_classes = torch.max(aggregated_probabilities, dim=1)\n",
    "\n",
    "    loss = criterion(aggregated_probabilities, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "\n",
    "  # Validation loop\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (frames, labels) in enumerate(test_dataloader):\n",
    "\n",
    "      # reshaping features for model input\n",
    "      batch_size, num_frames, channels, height, width = frames.size()\n",
    "      reshaped_frames = frames.view(\n",
    "          batch_size * num_frames, channels, height, width)\n",
    "\n",
    "      outputs = model(reshaped_frames)\n",
    "\n",
    "      # Reshape output to split batch and frame dimensions\n",
    "      reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "      probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "      # Aggregate predictions for each video\n",
    "      aggregated_probabilities = probabilities.mean(dim=1)\n",
    "      # Get the predicted class for each video\n",
    "      _, predicted_classes = torch.max(aggregated_probabilities, dim=1)\n",
    "\n",
    "      loss = criterion(aggregated_probabilities, labels)\n",
    "\n",
    "      val_loss += loss.item()\n",
    "      # _, predicted = torch.max(outputs, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted_classes == labels).sum().item()\n",
    "\n",
    "  # Calculate validation accuracy\n",
    "  val_loss /= len(test_dataloader.dataset)\n",
    "  val_accuracy = 100. * correct / total\n",
    "\n",
    "  # Print validation results\n",
    "  print(f\"Validation Results - Epoch {epoch+1}:\")\n",
    "  print(f\"Validation Loss: {val_loss:.4f} | Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "  # Save training and validation losses\n",
    "  train_losses.append(train_loss/len(train_dataloader.dataset))\n",
    "  val_losses.append(val_loss)\n",
    "\n",
    "  # Save model weights after every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    torch.save(model.state_dict(), model_weight_file)\n",
    "    print(f\"Epoch {epoch+1}: Model weights saved as {model_weight_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the video frames\n",
    "# def preprocess_frame(frame):\n",
    "#   frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "#   frame = transform(frame)\n",
    "#   return frame\n",
    "\n",
    "# # Function to classify video\n",
    "\n",
    "\n",
    "# def classify_video(video_path, threshold=0.5):\n",
    "#   cap = cv2.VideoCapture(video_path)\n",
    "#   frames = []\n",
    "#   while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#       break\n",
    "#     frame = preprocess_frame(frame)\n",
    "#     frames.append(frame)\n",
    "\n",
    "#   cap.release()\n",
    "\n",
    "#   # Convert frames to tensor and add batch dimension\n",
    "#   frames_tensor = torch.stack(frames).unsqueeze(0)\n",
    "\n",
    "#   # Pass frames through the model\n",
    "#   with torch.no_grad():\n",
    "#     model.eval()\n",
    "\n",
    "#     # Reshape frames for model input\n",
    "#     batch_size, num_frames, channels, height, width = frames_tensor.size()\n",
    "#     reshaped_frames = frames_tensor.view(\n",
    "#         batch_size * num_frames, channels, height, width)\n",
    "\n",
    "#     # Get model outputs\n",
    "#     outputs = model(reshaped_frames)\n",
    "\n",
    "#     # Reshape output to split batch and frame dimensions\n",
    "#     reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "#     probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "\n",
    "#     # Calculate the overall probability of violence across all frames\n",
    "#     overall_probability = probabilities[:, :, 1].mean()*100\n",
    "\n",
    "#     # Determine if the overall probability exceeds the threshold\n",
    "#     if overall_probability > threshold:\n",
    "#       prediction = \"Violence\"\n",
    "#     else:\n",
    "#       prediction = \"Non-violence\"\n",
    "\n",
    "#     return overall_probability.item(), prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the video frames\n",
    "# frames = [10, 3, 224, 224]\n",
    "def preprocess_frame(frames):\n",
    "  # num_frame = frames.shape[0]\n",
    "  frame_stack = []\n",
    "  for frame in frames:\n",
    "    frame = transform(frame)\n",
    "    frame_stack.append(frame)\n",
    "  return frame_stack\n",
    "\n",
    "# frame_stack = [10, 3, 224, 224]\n",
    "# Function to classify video\n",
    "\n",
    "\n",
    "def classify_video(frame_stack, threshold=0.5):\n",
    "\n",
    "  # Convert frames to tensor and add batch dimension\n",
    "  frames_tensor = torch.stack(frame_stack).unsqueeze(0)\n",
    "\n",
    "  # Pass frames through the model\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    # Reshape frames for model input\n",
    "    batch_size, num_frames, channels, height, width = frames_tensor.size()\n",
    "    reshaped_frames = frames_tensor.view(\n",
    "        batch_size * num_frames, channels, height, width)\n",
    "\n",
    "    # Get model outputs\n",
    "    outputs = model(reshaped_frames)\n",
    "\n",
    "    # Reshape output to split batch and frame dimensions\n",
    "    reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "    probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "\n",
    "    # Calculate the overall probability of violence across all frames\n",
    "    overall_probability = probabilities[:, :, 1].mean()*100\n",
    "\n",
    "    # Determine if the overall probability exceeds the threshold\n",
    "    if overall_probability > threshold:\n",
    "      prediction = \"Violence\"\n",
    "    else:\n",
    "      prediction = \"Non-violence\"\n",
    "\n",
    "    return overall_probability.item(), prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = r\"Dataset\\violent\\cam1\\47.mp4\"  # Path to your test video\n",
    "# result = classify_video(video_path)\n",
    "# print(\"Classification:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 240, 240, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = np.load('arrays_data.npz')['arr1']\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 1.3413,  1.3242,  1.2728,  ...,  1.0844,  1.1015,  1.1015],\n",
       "          [ 1.3242,  1.3413,  1.3070,  ...,  1.1015,  1.1015,  1.1015],\n",
       "          [ 1.2728,  1.2899,  1.2899,  ...,  1.0502,  1.0502,  1.0502],\n",
       "          ...,\n",
       "          [-1.4500, -1.4158, -1.3987,  ..., -0.3541, -0.3883, -0.4054],\n",
       "          [-1.4500, -1.4329, -1.3815,  ..., -0.3883, -0.4226, -0.4226],\n",
       "          [-1.4500, -1.4672, -1.4158,  ..., -0.3712, -0.4226, -0.4911]],\n",
       " \n",
       "         [[ 1.6583,  1.6408,  1.6408,  ...,  1.3431,  1.3256,  1.3256],\n",
       "          [ 1.6583,  1.6583,  1.6758,  ...,  1.3606,  1.3256,  1.3256],\n",
       "          [ 1.6758,  1.6933,  1.6933,  ...,  1.3782,  1.3782,  1.3782],\n",
       "          ...,\n",
       "          [-1.3704, -1.3354, -1.3179,  ..., -0.1450, -0.1800, -0.1975],\n",
       "          [-1.4055, -1.3880, -1.3704,  ..., -0.1450, -0.1975, -0.2150],\n",
       "          [-1.4055, -1.4230, -1.4055,  ..., -0.1099, -0.1975, -0.3025]],\n",
       " \n",
       "         [[ 1.8557,  1.8383,  1.8208,  ...,  1.4025,  1.4025,  1.4025],\n",
       "          [ 1.8557,  1.8557,  1.8557,  ...,  1.4200,  1.4025,  1.4025],\n",
       "          [ 1.8034,  1.8208,  1.8208,  ...,  1.4200,  1.4200,  1.4200],\n",
       "          ...,\n",
       "          [-0.5495, -0.5147, -0.4624,  ...,  0.0431,  0.0082, -0.0092],\n",
       "          [-0.5670, -0.5321, -0.4450,  ...,  0.0082, -0.0267, -0.0267],\n",
       "          [-0.5670, -0.5670, -0.4798,  ...,  0.0256, -0.0267, -0.0964]]]),\n",
       " tensor([[[ 1.2899,  1.2899,  1.2899,  ...,  1.0844,  1.0502,  1.0502],\n",
       "          [ 1.2385,  1.2385,  1.2728,  ...,  1.0844,  1.0331,  1.0331],\n",
       "          [ 1.2557,  1.2385,  1.2728,  ...,  1.0502,  1.0502,  1.0502],\n",
       "          ...,\n",
       "          [-1.4672, -1.4500, -1.4158,  ..., -0.6965, -0.4911, -0.3541],\n",
       "          [-1.4158, -1.4158, -1.3815,  ..., -0.7137, -0.5253, -0.4226],\n",
       "          [-1.4329, -1.4158, -1.3815,  ..., -0.7137, -0.5767, -0.4739]],\n",
       " \n",
       "         [[ 1.6933,  1.6933,  1.6933,  ...,  1.3256,  1.3431,  1.3431],\n",
       "          [ 1.6583,  1.6583,  1.6758,  ...,  1.3256,  1.3081,  1.3081],\n",
       "          [ 1.7108,  1.6933,  1.6758,  ...,  1.3431,  1.3431,  1.3431],\n",
       "          ...,\n",
       "          [-1.4755, -1.4755, -1.4230,  ..., -0.4601, -0.2500, -0.1099],\n",
       "          [-1.4405, -1.4405, -1.3880,  ..., -0.4601, -0.2325, -0.1275],\n",
       "          [-1.4580, -1.4405, -1.3880,  ..., -0.4601, -0.2850, -0.1625]],\n",
       " \n",
       "         [[ 1.8557,  1.8557,  1.8557,  ...,  1.4722,  1.4722,  1.4722],\n",
       "          [ 1.8208,  1.8208,  1.8208,  ...,  1.4722,  1.4548,  1.4548],\n",
       "          [ 1.8208,  1.8034,  1.7685,  ...,  1.4374,  1.4374,  1.4374],\n",
       "          ...,\n",
       "          [-0.5844, -0.5670, -0.5670,  ..., -0.2707, -0.0964,  0.0256],\n",
       "          [-0.5321, -0.5321, -0.5147,  ..., -0.3578, -0.1312, -0.0267],\n",
       "          [-0.5495, -0.5321, -0.4973,  ..., -0.3753, -0.2010, -0.0790]]]),\n",
       " tensor([[[ 1.3242,  1.3413,  1.3584,  ...,  1.0502,  1.0502,  1.0159],\n",
       "          [ 1.3070,  1.3070,  1.3413,  ...,  1.0502,  1.0502,  1.0502],\n",
       "          [ 1.2557,  1.2557,  1.2899,  ...,  1.0502,  1.0673,  1.0673],\n",
       "          ...,\n",
       "          [-1.5014, -1.4500, -1.4329,  ..., -0.4911, -0.7822, -0.7822],\n",
       "          [-1.4672, -1.4158, -1.3815,  ..., -0.4739, -0.7479, -0.7137],\n",
       "          [-1.4843, -1.4158, -1.3644,  ..., -0.4397, -0.7479, -0.7137]],\n",
       " \n",
       "         [[ 1.6408,  1.6583,  1.6758,  ...,  1.3606,  1.3782,  1.3431],\n",
       "          [ 1.6583,  1.6583,  1.6583,  ...,  1.3606,  1.3782,  1.3782],\n",
       "          [ 1.7108,  1.7108,  1.6933,  ...,  1.3431,  1.3782,  1.3782],\n",
       "          ...,\n",
       "          [-1.4405, -1.4055, -1.3704,  ..., -0.1975, -0.4951, -0.4951],\n",
       "          [-1.4405, -1.3880, -1.3354,  ..., -0.2150, -0.5476, -0.4951],\n",
       "          [-1.4580, -1.3880, -1.3354,  ..., -0.1975, -0.5301, -0.5126]],\n",
       " \n",
       "         [[ 1.8383,  1.8731,  1.9254,  ...,  1.5245,  1.4897,  1.4722],\n",
       "          [ 1.8383,  1.8383,  1.8905,  ...,  1.5245,  1.4722,  1.4722],\n",
       "          [ 1.8557,  1.8557,  1.8557,  ...,  1.4722,  1.4200,  1.4200],\n",
       "          ...,\n",
       "          [-0.4798, -0.4450, -0.4275,  ..., -0.0267, -0.3230, -0.3230],\n",
       "          [-0.5321, -0.4973, -0.4624,  ..., -0.0441, -0.3578, -0.3055],\n",
       "          [-0.5844, -0.5147, -0.4450,  ..., -0.0267, -0.3578, -0.3230]]]),\n",
       " tensor([[[ 1.2385,  1.2385,  1.2557,  ...,  1.0673,  1.0673,  1.0502],\n",
       "          [ 1.2385,  1.2557,  1.2557,  ...,  1.0502,  1.0502,  1.0502],\n",
       "          [ 1.2385,  1.2557,  1.2899,  ...,  1.0502,  1.0502,  1.0502],\n",
       "          ...,\n",
       "          [-1.4329, -1.3815, -1.3644,  ..., -0.3541, -0.2856, -0.6109],\n",
       "          [-1.4158, -1.4158, -1.3815,  ..., -0.4054, -0.4226, -0.8507],\n",
       "          [-1.4329, -1.4500, -1.4329,  ..., -0.4397, -0.4054, -0.7822]],\n",
       " \n",
       "         [[ 1.6758,  1.6758,  1.7108,  ...,  1.3782,  1.3782,  1.3606],\n",
       "          [ 1.6758,  1.6933,  1.7108,  ...,  1.3606,  1.3606,  1.3606],\n",
       "          [ 1.6758,  1.7108,  1.6933,  ...,  1.3606,  1.3606,  1.3606],\n",
       "          ...,\n",
       "          [-1.4055, -1.3704, -1.3529,  ..., -0.1625, -0.1099, -0.4426],\n",
       "          [-1.3880, -1.3880, -1.3704,  ..., -0.1625, -0.1275, -0.5826],\n",
       "          [-1.4055, -1.4230, -1.4055,  ..., -0.1975, -0.1275, -0.4776]],\n",
       " \n",
       "         [[ 1.8731,  1.8731,  1.8557,  ...,  1.3502,  1.3851,  1.3677],\n",
       "          [ 1.8731,  1.8905,  1.8557,  ...,  1.3328,  1.3677,  1.3677],\n",
       "          [ 1.8383,  1.8557,  1.8557,  ...,  1.3677,  1.3677,  1.3677],\n",
       "          ...,\n",
       "          [-0.4450, -0.4101, -0.3927,  ..., -0.0441,  0.0082, -0.3230],\n",
       "          [-0.4275, -0.4275, -0.4101,  ..., -0.0267, -0.0441, -0.4973],\n",
       "          [-0.4450, -0.4624, -0.4450,  ..., -0.0615, -0.0267, -0.3927]]]),\n",
       " tensor([[[ 1.2728,  1.2214,  1.2214,  ...,  1.0159,  1.0159,  1.0159],\n",
       "          [ 1.2728,  1.2728,  1.2728,  ...,  1.0159,  1.0159,  1.0159],\n",
       "          [ 1.2899,  1.3070,  1.2899,  ...,  1.0159,  1.0502,  1.0502],\n",
       "          ...,\n",
       "          [-1.4672, -1.4329, -1.3987,  ..., -0.3198, -0.2856, -0.5424],\n",
       "          [-1.4500, -1.4500, -1.4500,  ..., -0.3712, -0.3369, -0.6794],\n",
       "          [-1.5185, -1.5185, -1.5014,  ..., -0.3712, -0.3198, -0.7137]],\n",
       " \n",
       "         [[ 1.6583,  1.6232,  1.6232,  ...,  1.3782,  1.3606,  1.3606],\n",
       "          [ 1.6583,  1.6583,  1.6583,  ...,  1.3782,  1.3606,  1.3606],\n",
       "          [ 1.6583,  1.6758,  1.6933,  ...,  1.3782,  1.3606,  1.3606],\n",
       "          ...,\n",
       "          [-1.3880, -1.3354, -1.3179,  ..., -0.0924, -0.0224, -0.2850],\n",
       "          [-1.3704, -1.3704, -1.3354,  ..., -0.1275, -0.0749, -0.4251],\n",
       "          [-1.4230, -1.4230, -1.3880,  ..., -0.1099, -0.0749, -0.4601]],\n",
       " \n",
       "         [[ 1.9080,  1.8731,  1.8731,  ...,  1.3677,  1.4025,  1.4025],\n",
       "          [ 1.8905,  1.9080,  1.9080,  ...,  1.3677,  1.4025,  1.4025],\n",
       "          [ 1.8383,  1.8557,  1.8557,  ...,  1.3677,  1.3677,  1.3677],\n",
       "          ...,\n",
       "          [-0.6367, -0.5844, -0.4973,  ...,  0.0082,  0.0953, -0.1487],\n",
       "          [-0.5844, -0.5670, -0.4798,  ..., -0.0267,  0.0431, -0.2881],\n",
       "          [-0.6541, -0.6367, -0.5321,  ..., -0.0092,  0.0605, -0.3230]]]),\n",
       " tensor([[[ 1.2385,  1.2728,  1.2899,  ...,  1.0844,  1.0502,  1.0502],\n",
       "          [ 1.2385,  1.2728,  1.2728,  ...,  1.0502,  1.0502,  1.0502],\n",
       "          [ 1.2728,  1.2728,  1.2728,  ...,  1.0331,  1.0502,  1.0673],\n",
       "          ...,\n",
       "          [-1.3987, -1.3815, -1.3987,  ..., -0.4226, -0.6965, -0.7822],\n",
       "          [-1.3644, -1.3815, -1.3987,  ..., -0.4054, -0.7137, -0.7650],\n",
       "          [-1.4158, -1.4329, -1.3815,  ..., -0.4397, -0.7993, -0.9020]],\n",
       " \n",
       "         [[ 1.6583,  1.6758,  1.6933,  ...,  1.3256,  1.3431,  1.3431],\n",
       "          [ 1.6583,  1.6758,  1.6758,  ...,  1.3081,  1.3431,  1.3431],\n",
       "          [ 1.6758,  1.6758,  1.6758,  ...,  1.3256,  1.3431,  1.3606],\n",
       "          ...,\n",
       "          [-1.3529, -1.3529, -1.3354,  ..., -0.1275, -0.4076, -0.5126],\n",
       "          [-1.3354, -1.3704, -1.3354,  ..., -0.0749, -0.3725, -0.4251],\n",
       "          [-1.3880, -1.3880, -1.3354,  ..., -0.1099, -0.4601, -0.5476]],\n",
       " \n",
       "         [[ 1.8208,  1.8383,  1.8557,  ...,  1.4722,  1.4722,  1.4722],\n",
       "          [ 1.8208,  1.8383,  1.8383,  ...,  1.4548,  1.4722,  1.4722],\n",
       "          [ 1.8383,  1.8383,  1.8383,  ...,  1.4200,  1.4374,  1.4548],\n",
       "          ...,\n",
       "          [-0.5147, -0.4973, -0.3927,  ...,  0.0082, -0.2532, -0.3404],\n",
       "          [-0.4450, -0.4450, -0.3578,  ...,  0.0953, -0.2010, -0.2532],\n",
       "          [-0.4624, -0.4624, -0.3404,  ...,  0.0779, -0.2881, -0.3753]]]),\n",
       " tensor([[[ 1.3413,  1.3413,  1.3413,  ...,  1.1015,  1.0844,  1.0844],\n",
       "          [ 1.3413,  1.3413,  1.3413,  ...,  1.0844,  1.0673,  1.0673],\n",
       "          [ 1.2899,  1.2899,  1.3070,  ...,  1.0159,  1.0159,  1.0159],\n",
       "          ...,\n",
       "          [-1.4500, -1.4158, -1.4158,  ..., -0.6281, -0.7993, -0.6281],\n",
       "          [-1.4672, -1.4158, -1.3987,  ..., -0.7308, -0.7650, -0.5767],\n",
       "          [-1.4500, -1.3987, -1.3473,  ..., -0.7650, -0.7308, -0.5938]],\n",
       " \n",
       "         [[ 1.6758,  1.6758,  1.6758,  ...,  1.3256,  1.3431,  1.3431],\n",
       "          [ 1.6758,  1.6758,  1.6758,  ...,  1.3256,  1.3431,  1.3431],\n",
       "          [ 1.7108,  1.7108,  1.6933,  ...,  1.3606,  1.3606,  1.3606],\n",
       "          ...,\n",
       "          [-1.3704, -1.3354, -1.3529,  ..., -0.3375, -0.5126, -0.3375],\n",
       "          [-1.3880, -1.3354, -1.3354,  ..., -0.4426, -0.4776, -0.2850],\n",
       "          [-1.3704, -1.3179, -1.2829,  ..., -0.4951, -0.4601, -0.3200]],\n",
       " \n",
       "         [[ 1.7860,  1.7860,  1.8208,  ...,  1.3677,  1.4025,  1.4025],\n",
       "          [ 1.7860,  1.7860,  1.8208,  ...,  1.3851,  1.4025,  1.4025],\n",
       "          [ 1.7860,  1.7860,  1.7860,  ...,  1.4374,  1.4374,  1.4374],\n",
       "          ...,\n",
       "          [-0.5147, -0.4798, -0.4450,  ..., -0.1835, -0.3404, -0.1835],\n",
       "          [-0.5670, -0.4973, -0.4275,  ..., -0.2532, -0.2881, -0.0964],\n",
       "          [-0.5495, -0.4798, -0.3753,  ..., -0.2881, -0.2532, -0.0964]]]),\n",
       " tensor([[[ 1.3242,  1.3413,  1.3070,  ...,  1.1187,  1.1015,  1.1015],\n",
       "          [ 1.3413,  1.3413,  1.3070,  ...,  1.1187,  1.1015,  1.1015],\n",
       "          [ 1.3413,  1.3070,  1.2899,  ...,  1.1015,  1.0844,  1.0844],\n",
       "          ...,\n",
       "          [-1.4500, -1.4329, -1.4158,  ..., -0.3883, -0.4226, -0.5082],\n",
       "          [-1.4329, -1.3815, -1.3815,  ..., -0.4054, -0.4568, -0.5253],\n",
       "          [-1.4329, -1.3644, -1.3815,  ..., -0.4397, -0.5253, -0.5767]],\n",
       " \n",
       "         [[ 1.6232,  1.6408,  1.6583,  ...,  1.3431,  1.3606,  1.3606],\n",
       "          [ 1.6408,  1.6408,  1.6583,  ...,  1.3431,  1.3606,  1.3606],\n",
       "          [ 1.6933,  1.6758,  1.6408,  ...,  1.3256,  1.3431,  1.3431],\n",
       "          ...,\n",
       "          [-1.3529, -1.3354, -1.3354,  ..., -0.1275, -0.1800, -0.2500],\n",
       "          [-1.3880, -1.3179, -1.3004,  ..., -0.1450, -0.1800, -0.2675],\n",
       "          [-1.3880, -1.3179, -1.3004,  ..., -0.1800, -0.2675, -0.3200]],\n",
       " \n",
       "         [[ 1.9428,  1.9603,  1.9603,  ...,  1.3851,  1.4200,  1.4200],\n",
       "          [ 1.9428,  1.9428,  1.9428,  ...,  1.3851,  1.4200,  1.4200],\n",
       "          [ 1.8905,  1.8557,  1.8731,  ...,  1.3677,  1.3677,  1.3677],\n",
       "          ...,\n",
       "          [-0.6193, -0.5844, -0.4798,  ...,  0.0082, -0.0441, -0.1138],\n",
       "          [-0.5844, -0.5321, -0.4101,  ..., -0.0441, -0.1138, -0.1835],\n",
       "          [-0.5844, -0.5147, -0.3927,  ..., -0.0964, -0.2010, -0.2707]]]),\n",
       " tensor([[[ 1.3070,  1.3070,  1.3070,  ...,  1.0159,  1.0159,  1.0159],\n",
       "          [ 1.3070,  1.3070,  1.3070,  ...,  1.0159,  1.0159,  1.0502],\n",
       "          [ 1.2899,  1.2899,  1.2899,  ...,  1.0159,  1.0331,  1.0331],\n",
       "          ...,\n",
       "          [-1.3987, -1.3644, -1.4158,  ..., -0.5253, -0.5253, -0.4397],\n",
       "          [-1.3987, -1.4158, -1.4672,  ..., -0.5253, -0.4911, -0.4226],\n",
       "          [-1.4843, -1.4672, -1.4500,  ..., -0.5424, -0.5253, -0.4739]],\n",
       " \n",
       "         [[ 1.6758,  1.6758,  1.6758,  ...,  1.3782,  1.3782,  1.3782],\n",
       "          [ 1.6758,  1.6758,  1.6758,  ...,  1.3782,  1.3782,  1.3957],\n",
       "          [ 1.6583,  1.6583,  1.6758,  ...,  1.3606,  1.3431,  1.3431],\n",
       "          ...,\n",
       "          [-1.4055, -1.3704, -1.3529,  ..., -0.3200, -0.3200, -0.2325],\n",
       "          [-1.3354, -1.3179, -1.3179,  ..., -0.3375, -0.3025, -0.2150],\n",
       "          [-1.4055, -1.3704, -1.3004,  ..., -0.3375, -0.3200, -0.2850]],\n",
       " \n",
       "         [[ 1.8557,  1.8557,  1.8557,  ...,  1.3677,  1.3328,  1.3328],\n",
       "          [ 1.8557,  1.8557,  1.8383,  ...,  1.3677,  1.3328,  1.3502],\n",
       "          [ 1.8034,  1.8034,  1.7685,  ...,  1.4025,  1.3502,  1.3502],\n",
       "          ...,\n",
       "          [-0.5495, -0.4973, -0.4450,  ..., -0.1312, -0.1138, -0.0267],\n",
       "          [-0.4973, -0.4973, -0.4798,  ..., -0.1312, -0.0790,  0.0082],\n",
       "          [-0.5844, -0.5495, -0.4624,  ..., -0.1487, -0.0964, -0.0441]]]),\n",
       " tensor([[[ 1.2214,  1.2214,  1.2385,  ...,  1.1015,  1.1015,  1.1015],\n",
       "          [ 1.2557,  1.2557,  1.2557,  ...,  1.1187,  1.1015,  1.1015],\n",
       "          [ 1.2899,  1.2899,  1.2557,  ...,  1.1015,  1.0844,  1.0844],\n",
       "          ...,\n",
       "          [-1.4500, -1.3987, -1.3644,  ..., -0.5253, -0.5082, -0.3883],\n",
       "          [-1.3644, -1.3302, -1.3302,  ..., -0.5596, -0.5253, -0.4397],\n",
       "          [-1.3644, -1.3473, -1.3644,  ..., -0.6452, -0.6452, -0.5596]],\n",
       " \n",
       "         [[ 1.6758,  1.6758,  1.6933,  ...,  1.3256,  1.3081,  1.3081],\n",
       "          [ 1.7108,  1.7108,  1.7108,  ...,  1.3431,  1.3081,  1.3081],\n",
       "          [ 1.6933,  1.6933,  1.7108,  ...,  1.3606,  1.3431,  1.3431],\n",
       "          ...,\n",
       "          [-1.3354, -1.2829, -1.2479,  ..., -0.2325, -0.2150, -0.0924],\n",
       "          [-1.3354, -1.3179, -1.2829,  ..., -0.2325, -0.1800, -0.1099],\n",
       "          [-1.3704, -1.3354, -1.3354,  ..., -0.3025, -0.3025, -0.2325]],\n",
       " \n",
       "         [[ 1.7860,  1.7860,  1.8034,  ...,  1.4025,  1.4548,  1.4722],\n",
       "          [ 1.8208,  1.8208,  1.8208,  ...,  1.4200,  1.4548,  1.4548],\n",
       "          [ 1.8208,  1.8208,  1.8208,  ...,  1.3851,  1.3677,  1.3677],\n",
       "          ...,\n",
       "          [-0.4798, -0.4275, -0.3927,  ..., -0.1138, -0.1138, -0.0092],\n",
       "          [-0.4973, -0.4624, -0.4450,  ..., -0.1138, -0.0964,  0.0082],\n",
       "          [-0.5147, -0.4973, -0.4973,  ..., -0.2010, -0.2010, -0.1138]]])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_frame(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4199000895023346, 'Non-violence')\n"
     ]
    }
   ],
   "source": [
    "preprocessed_frame = preprocess_frame(frames)\n",
    "result = classify_video(preprocessed_frame)\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server listening on port 8765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<websockets.legacy.server.WebSocketServer at 0x1b2b7a0a1a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a client just connected\n",
      "Latency (ms) since previous message: 98.37913513183594\n",
      "Latency (ms) since previous message: 103.45745086669922\n",
      "Latency (ms) since previous message: 98.68383407592773\n",
      "Latency (ms) since previous message: 106.19568824768066\n",
      "Latency (ms) since previous message: 101.00793838500977\n",
      "Latency (ms) since previous message: 89.25676345825195\n",
      "Latency (ms) since previous message: 99.40552711486816\n",
      "Latency (ms) since previous message: 114.78781700134277\n",
      "Latency (ms) since previous message: 221.72808647155762\n",
      "(10, 240, 240, 3)\n",
      "[0.5579568147659302, 'Non-violence']\n",
      "Latency (ms) since previous message: 998.2092380523682\n",
      "Latency (ms) since previous message: 1002.3055076599121\n",
      "Latency (ms) since previous message: 995.265007019043\n",
      "Latency (ms) since previous message: 1007.148027420044\n",
      "Latency (ms) since previous message: 996.1097240447998\n",
      "Latency (ms) since previous message: 995.8040714263916\n",
      "Latency (ms) since previous message: 998.9709854125977\n",
      "Latency (ms) since previous message: 1005.8352947235107\n",
      "Latency (ms) since previous message: 994.8854446411133\n",
      "Latency (ms) since previous message: 999.8598098754883\n",
      "(10, 240, 240, 3)\n",
      "[0.5598028302192688, 'Non-violence']\n",
      "Latency (ms) since previous message: 998.1963634490967\n",
      "Latency (ms) since previous message: 1006.4024925231934\n",
      "Latency (ms) since previous message: 993.6590194702148\n",
      "Latency (ms) since previous message: 1003.9453506469727\n",
      "Latency (ms) since previous message: 996.2763786315918\n",
      "Latency (ms) since previous message: 1007.1399211883545\n",
      "Latency (ms) since previous message: 991.647481918335\n",
      "Latency (ms) since previous message: 1007.5206756591797\n",
      "Latency (ms) since previous message: 996.7772960662842\n",
      "Latency (ms) since previous message: 1003.3633708953857\n",
      "(10, 240, 240, 3)\n",
      "[0.572348028421402, 'Non-violence']\n",
      "Latency (ms) since previous message: 995.6951141357422\n",
      "Latency (ms) since previous message: 999.9358654022217\n",
      "Latency (ms) since previous message: 999.3441104888916\n",
      "Latency (ms) since previous message: 998.8641738891602\n",
      "Latency (ms) since previous message: 1007.8184604644775\n",
      "Latency (ms) since previous message: 997.1938133239746\n",
      "Latency (ms) since previous message: 995.5718517303467\n",
      "Latency (ms) since previous message: 1008.873701095581\n",
      "Latency (ms) since previous message: 1001.5511512756348\n",
      "Latency (ms) since previous message: 994.4431781768799\n",
      "(10, 240, 240, 3)\n",
      "[0.5725535750389099, 'Non-violence']\n",
      "Latency (ms) since previous message: 997.3499774932861\n",
      "Latency (ms) since previous message: 1001.0738372802734\n",
      "Latency (ms) since previous message: 999.753475189209\n",
      "Latency (ms) since previous message: 995.7692623138428\n",
      "Latency (ms) since previous message: 1001.3225078582764\n",
      "Latency (ms) since previous message: 1008.6715221405029\n",
      "Latency (ms) since previous message: 999.182939529419\n",
      "Latency (ms) since previous message: 999.7401237487793\n",
      "Latency (ms) since previous message: 993.3242797851562\n"
     ]
    }
   ],
   "source": [
    "import websockets\n",
    "import asyncio\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "PORT = 8765\n",
    "print(\"server listening on port \" + str(PORT))\n",
    "\n",
    "\n",
    "async def main(websocket):\n",
    "  print(\"a client just connected\")\n",
    "  prev_message_time = None\n",
    "  pixel_data_store = []\n",
    "\n",
    "  global test_frames\n",
    "  try:\n",
    "    async for message in websocket:\n",
    "      current_time = time.time()\n",
    "      if prev_message_time is not None:\n",
    "        latency_ms = (current_time - prev_message_time) * 1000\n",
    "        print(\"Latency (ms) since previous message:\", latency_ms)\n",
    "      prev_message_time = current_time\n",
    "\n",
    "      pixel_data = np.frombuffer(message, dtype=np.uint8)\n",
    "\n",
    "      # pixel_data = pixel_data.reshape(240, 240, 4)[:, :, :3]\n",
    "\n",
    "      # pixel_data = np.array([\n",
    "      #     pixel_data[:, :, 0],\n",
    "      #     pixel_data[:, :, 1],\n",
    "      #     pixel_data[:, :, 2]\n",
    "      # ])\n",
    "\n",
    "      pixel_data_store.append(pixel_data)\n",
    "\n",
    "      if len(pixel_data_store) < 10:\n",
    "        continue\n",
    "\n",
    "      # 🤖\n",
    "      frames = np.array(pixel_data_store).reshape(10, 240, 240, 4)[:, :, :, :3]\n",
    "      # frames = np.array(pixel_data_store)\n",
    "      print(frames.shape)\n",
    "\n",
    "      # np.savez('arrays_data.npz', arr1=frames)\n",
    "\n",
    "      # ⭐\n",
    "      preprocessed_frame = preprocess_frame(frames)\n",
    "      result = classify_video(preprocessed_frame)\n",
    "      result = list(result)\n",
    "      result[0] = 1 - result[0]\n",
    "      print(str(result))\n",
    "\n",
    "      pixel_data_store = []\n",
    "  except websockets.exceptions.ConnectionClosed as e:\n",
    "    print(\"a client just disconnected\", e)\n",
    "\n",
    "\n",
    "start_server = websockets.serve(main, \"localhost\", PORT)\n",
    "asyncio.get_event_loop().run_until_complete(start_server)\n",
    "asyncio.get_event_loop().run_forever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
